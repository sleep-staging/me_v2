diff --git a/shhs_preprocess/delete.py b/shhs_preprocess/delete.py
deleted file mode 100644
index a3ff253..0000000
--- a/shhs_preprocess/delete.py
+++ /dev/null
@@ -1,464 +0,0 @@
-##!/usr/bin/env python
-## coding: utf-8
-#
-## In[1]:
-##%%
-#
-#import re, datetime, operator, logging, sys
-#import numpy as np
-#from collections import namedtuple
-#
-#
-## In[2]:
-#
-#
-#import os
-#import numpy as np
-#
-#import argparse
-#import glob
-#import math
-#import ntpath
-#
-#import shutil
-#import urllib
-## import urllib2
-#
-#from datetime import datetime
-#import warnings
-#warnings.filterwarnings("ignore")
-#
-#import pandas as pd
-#from mne.io import concatenate_raws, read_raw_edf
-##import dhedfreader
-#import xml.etree.ElementTree as ET
-#
-#
-#
-#
-#
-#EVENT_CHANNEL = 'EDF Annotations'
-## log = logging.getLogger(__name__)
-#
-#class EDFEndOfData(Exception): pass
-#
-#def tal(tal_str):
-#    '''Return a list with (onset, duration, annotation) tuples for an EDF+ TAL
-#  stream.
-#  '''
-#    exp = '(?P<onset>[+\-]\d+(?:\.\d*)?)' + '(?:\x15(?P<duration>\d+(?:\.\d*)?))?' + '(\x14(?P<annotation>[^\x00]*))?' + '(?:\x14\x00)'
-#
-#    def annotation_to_list(annotation):
-#        return str(annotation.encode('utf-8')).split('\x14') if annotation else []
-#
-#    def parse(dic):
-#        return (
-#      float(dic['onset']),
-#      float(dic['duration']) if dic['duration'] else 0.,
-#      annotation_to_list(dic['annotation']))
-#
-#    return [parse(m.groupdict()) for m in re.finditer(exp, tal_str)]
-#
-#
-#def edf_header(f):
-#    h = {}
-#    assert f.tell() == 0  # check file position
-#    assert f.read(8) == '0       '
-#
-#    # recording info)
-#    h['local_subject_id'] = f.read(80).strip()
-#    h['local_recording_id'] = f.read(80).strip()
-#
-#    # parse timestamp
-#    (day, month, year) = [int(x) for x in re.findall('(\d+)', f.read(8))]
-#    (hour, minute, sec)= [int(x) for x in re.findall('(\d+)', f.read(8))]
-#    h['date_time'] = str(datetime.datetime(year + 2000, month, day,
-#    hour, minute, sec))
-#
-#    # misc
-#    header_nbytes = int(f.read(8))
-#    subtype = f.read(44)[:5]
-#    h['EDF+'] = subtype in ['EDF+C', 'EDF+D']
-#    h['contiguous'] = subtype != 'EDF+D'
-#    h['n_records'] = int(f.read(8))
-#    h['record_length'] = float(f.read(8))  # in seconds
-#    nchannels = h['n_channels'] = int(f.read(4))
-#
-#    # read channel info
-#    channels = range(h['n_channels'])
-#    h['label'] = [f.read(16).strip() for n in channels]
-#    h['transducer_type'] = [f.read(80).strip() for n in channels]
-#    h['units'] = [f.read(8).strip() for n in channels]
-#    h['physical_min'] = np.asarray([float(f.read(8)) for n in channels])
-#    h['physical_max'] = np.asarray([float(f.read(8)) for n in channels])
-#    h['digital_min'] = np.asarray([float(f.read(8)) for n in channels])
-#    h['digital_max'] = np.asarray([float(f.read(8)) for n in channels])
-#    h['prefiltering'] = [f.read(80).strip() for n in channels]
-#    h['n_samples_per_record'] = [int(f.read(8)) for n in channels]
-#    f.read(32 * nchannels)  # reserved
-#
-#    #assert f.tell() == header_nbytes
-#    return h
-#
-#
-#class BaseEDFReader:
-#    def __init__(self, file):
-#        self.file = file
-#
-#
-#    def read_header(self):
-#        self.header = h = edf_header(self.file)
-#
-#        # calculate ranges for rescaling
-#        self.dig_min = h['digital_min']
-#        self.phys_min = h['physical_min']
-#        phys_range = h['physical_max'] - h['physical_min']
-#        dig_range = h['digital_max'] - h['digital_min']
-#        assert np.all(phys_range > 0)
-#        assert np.all(dig_range > 0)
-#        self.gain = phys_range / dig_range
-#
-#
-#    def read_raw_record(self):
-#        '''Read a record with data_2013 and return a list containing arrays with raw
-#        bytes.
-#        '''
-#        result = []
-#        for nsamp in self.header['n_samples_per_record']:
-#            samples = self.file.read(nsamp * 2)
-#            if len(samples) != nsamp * 2:
-#                raise EDFEndOfData
-#            result.append(samples)
-#        return result
-#
-#
-#    def convert_record(self, raw_record):
-#        '''Convert a raw record to a (time, signals, events) tuple based on
-#        information in the header.
-#        '''
-#        h = self.header
-#        dig_min, phys_min, gain = self.dig_min, self.phys_min, self.gain
-#        time = float('nan')
-#        signals = []
-#        events = []
-#        for (i, samples) in enumerate(raw_record):
-#            if h['label'][i] == EVENT_CHANNEL:
-#                ann = tal(samples)
-#                time = ann[0][0]
-#                events.extend(ann[1:])
-#                # print(i, samples)
-#                # exit()
-#            else:
-#                # 2-byte little-endian integers
-#                dig = np.fromstring(samples, '<i2').astype(np.float32)
-#                phys = (dig - dig_min[i]) * gain[i] + phys_min[i]
-#                signals.append(phys)
-#
-#        return time, signals, events
-#
-#
-#    def read_record(self):
-#        return self.convert_record(self.read_raw_record())
-#
-#
-#    def records(self):
-#        '''
-#        Record generator.
-#        '''
-#        try:
-#            while True:
-#                yield self.read_record()
-#        except EDFEndOfData:
-#            pass
-#
-#
-#def load_edf(edffile):
-#    '''Load an EDF+ file.
-#  Very basic reader for EDF and EDF+ files. While BaseEDFReader does support
-#  exotic features like non-homogeneous sample rates and loading only parts of
-#  the stream, load_edf expects a single fixed sample rate for all channels and
-#  tries to load the whole file.
-#  Parameters
-#  ----------
-#  edffile : file-like object or string
-#  Returns
-#  -------
-#  Named tuple with the fields:
-#    X : NumPy array with shape p by n.
-#      Raw recording of n samples in p dimensions.
-#    sample_rate : float
-#      The sample rate of the recording. Note that mixed sample-rates are not
-#      supported.
-#    sens_lab : list of length p with strings
-#      The labels of the sensors used to record X.
-#    time : NumPy array with length n
-#      The time offset in the recording for each sample.
-#    annotations : a list with tuples
-#      EDF+ annotations are stored in (start, duration, description) tuples.
-#      start : float
-#        Indicates the start of the event in seconds.
-#      duration : float
-#        Indicates the duration of the event in seconds.
-#      description : list with strings
-#        Contains (multiple?) descriptions of the annotation event.
-#  '''
-#    if isinstance(edffile, basestring):
-#        with open(edffile, 'rb') as f:
-#            return load_edf(f)  # convert filename to file
-#
-#    reader = BaseEDFReader(edffile)
-#    reader.read_header()
-#
-#    h = reader.header
-#    log.debug('EDF header: %s' % h)
-#
-#      # get sample rate info
-#    nsamp = np.unique(
-#        [n for (l, n) in zip(h['label'], h['n_samples_per_record'])
-#        if l != EVENT_CHANNEL])
-#    assert nsamp.size == 1, 'Multiple sample rates not supported!'
-#    sample_rate = float(nsamp[0]) / h['record_length']
-#
-#    rectime, X, annotations = zip(*reader.records())
-#    X = np.hstack(X)
-#    annotations = reduce(operator.add, annotations)
-#    chan_lab = [lab for lab in reader.header['label'] if lab != EVENT_CHANNEL]
-#
-#      # create timestamps
-#    if reader.header['contiguous']:
-#        time = np.arange(X.shape[1]) / sample_rate
-#    else:
-#        reclen = reader.header['record_length']
-#        within_rec_time = np.linspace(0, reclen, nsamp, endpoint=False)
-#        time = np.hstack([t + within_rec_time for t in rectime])
-#
-#    tup = namedtuple('EDF', 'X sample_rate chan_lab time annotations')
-#    return tup(X, sample_rate, chan_lab, time, annotations)
-#
-##%%
-## In[6]:
-#
-#
-#EPOCH_SEC_SIZE = 30
-#
-## data on GNODE 25 DATE: 06-12-21 (ALL 329 files of SHHS1)
-#
-#
-#data_dir = '/scratch/SLEEP_data/shhs/polysomnography/edfs/shhs1'
-#ann_dir = '/scratch/SLEEP_data/shhs/polysomnography/annotations-events-profusion/shhs1'
-#output_dir = '/scratch/SLEEP_data/shhs/output'
-#select_ch = 'EEG C4-A1'  #EEG (sec)	C3	A2  #EEG	C4	A1
-#
-#csv_path = '/scratch/SLEEP_data/selected_shhs1_files.txt'
-#if not os.path.exists(output_dir):
-#    os.mkdir(output_dir)
-#
-##ids = pd.read_csv("selected_shhs1_files.txt", header=None, names='a')
-#ids = pd.read_csv(csv_path, header=None)
-#ids = ids[0].values.tolist()
-#
-#edf_fnames = [os.path.join(data_dir, i + ".edf") for i in ids]
-#ann_fnames = [os.path.join(ann_dir,  i + "-profusion.xml") for i in ids]
-#
-#edf_fnames.sort()
-#ann_fnames.sort()
-#
-#edf_fnames = np.asarray(edf_fnames)
-#ann_fnames = np.asarray(ann_fnames)
-#
-##yahase 
-#for file_id in range(len(edf_fnames)):
-#    if os.path.exists(os.path.join(output_dir, edf_fnames[file_id].split('/')[-1])[:-4]+".npz"):
-#        continue
-#    print(edf_fnames[file_id])
-#    select_ch = 'EEG C4-A1'
-#    raw = read_raw_edf(edf_fnames[file_id], preload=True, stim_channel=None, verbose=None)
-#    sampling_rate = raw.info['sfreq']
-#    ch_type = select_ch.split(" ")[0]    # selecting EEG out of 'EEG C4-A1'
-#    select_ch = sorted([s for s in raw.info["ch_names"] if ch_type in s]) # this has 2 vals [EEG,EEG(sec)] and selecting 0th index
-#    print(select_ch)
-#    raw_ch_df = raw.to_data_frame(scalings=sampling_rate)[select_ch]
-#    print(raw_ch_df.shape)
-#    #raw_ch_df = raw_ch_df.to_frame()
-#    raw_ch_df.set_index(np.arange(len(raw_ch_df)))
-#    
-######################################################
-#
-##     # X load
-#    
-##     X = raw.get_data()
-##     if X.shape[0] == 16:
-##         X = X[[0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15], :]
-##     elif X.shape[0] == 15:
-##         X = X[[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14], :]
-##     X = X[[2,7], :]
-##     print("1")
-##     raw_ch_df = X.to_data_frame(scalings=sampling_rate)
-##     print("2")
-##     raw_ch_df = raw_ch_df.to_frame()
-##     print("3")
-##     raw_ch_df.set_index(np.arange(len(raw_ch_df)))
-##     print("4")
-##     print(abc)
-#
-####################################################
-#    labels = []
-#    # Read annotation and its header
-#    t = ET.parse(ann_fnames[file_id])
-#    r = t.getroot()
-#    faulty_File = 0
-#    for i in range(len(r[4])):
-#        lbl = int(r[4][i].text)
-#        if lbl == 4:  # make stages N3, N4 same as N3
-#            labels.append(3)
-#        elif lbl == 5:  # Assign label 4 for REM stage
-#            labels.append(4)
-#        else:
-#            labels.append(lbl)
-#        if lbl > 5:  # some files may contain labels > 5 BUT not the selected ones.
-#            faulty_File = 1
-#
-#    if faulty_File == 1:
-#        print( "============================== Faulty file ==================")
-#        continue
-#
-#    labels = np.asarray(labels)
-#
-#    # Remove movement and unknown stages if any
-#    raw_ch = raw_ch_df.values
-#    print(raw_ch.shape)
-#
-#    # Verify that we can split into 30-s epochs
-#    if len(raw_ch) % (EPOCH_SEC_SIZE * sampling_rate) != 0:
-#        raise Exception("Something wrong")
-#    n_epochs = len(raw_ch) / (EPOCH_SEC_SIZE * sampling_rate)
-#
-#    # Get epochs and their corresponding labels
-#    x = np.asarray(np.split(raw_ch, n_epochs)).astype(np.float32)
-#    y = labels.astype(np.int32)
-#
-#    print(x.shape)
-#    print(y.shape)
-#    assert len(x) == len(y)
-#
-#    # Select on sleep periods
-#    w_edge_mins = 30
-#    nw_idx = np.where(y != 0)[0]
-#    start_idx = nw_idx[0] - (w_edge_mins * 2)
-#    end_idx = nw_idx[-1] + (w_edge_mins * 2)
-#    if start_idx < 0: start_idx = 0
-#    if end_idx >= len(y): end_idx = len(y) - 1
-#    select_idx = np.arange(start_idx, end_idx + 1)
-#    print("Data before selection: {}, {}".format(x.shape, y.shape))
-#    x = x[select_idx]
-#    y = y[select_idx]
-#    print("Data after selection: {}, {}".format(x.shape, y.shape))
-#
-#    # Saving as numpy files
-#    filename = os.path.basename(edf_fnames[file_id]).replace(".edf",  ".npz")
-#    save_dict = {
-#        "x": x,
-#        "y": y,
-#        "fs": sampling_rate
-#    }
-#    np.savez(os.path.join(output_dir, filename), **save_dict)
-#    print(" ---------- Done this file ---------")
-#
-#
-## In[3]:
-
-
-import os
-import torch
-import numpy as np
-import argparse
-
-seed = 123
-np.random.seed(seed)
-
-
-# parser = argparse.ArgumentParser()
-
-# parser.add_argument("--dir", type=str, default="/scratch/SLEEP_data",
-#                     help="File path to the PSG and annotation files.")
-
-# args = parser.parse_args()
-
-dire = '/scratch/SLEEP_DATA'
-data_dir = os.path.join(dire, "output")    #on gnode27 = "numpy_subjects"
-output_dir = dire
-
-files = os.listdir(data_dir)
-files = np.array([os.path.join(data_dir, i) for i in files])
-files.sort()
-
-#print(files)
-
-######## pretext files##########
-pretext_files = list(np.random.choice(files,264,replace=False))    #change
-
-print("pretext files: ", len(pretext_files))
-
-# load files
-
-#X_train = np.load(pretext_files[0])["x"]
-#y_train = np.load(pretext_files[0])["y"]
-#c=0
-#for np_file in pretext_files[0:]:
-#    c =c+1
-#    print(c)
-#    print(os.path.basename(np_file))
-#    X_train = np.vstack((X_train, np.load(np_file)["x"]))
-#    y_train = np.append(y_train, np.load(np_file)["y"])
-#
-#
-#data_save = dict()
-#data_save["samples"] = torch.from_numpy(X_train.transpose(0, 2, 1))
-#data_save["labels"] = torch.from_numpy(y_train)
-
-#torch.save(data_save, os.path.join(output_dir, "pretext.pt"))
-
-######## training files ##########
-training_files = list(np.random.choice(sorted(list(set(files)-set(pretext_files))),31,replace=False))  #change
-
-print("\n =========================================== \n")
-print("training files: ", len(training_files))
-
-# load files
-#X_train = np.load(training_files[0])["x"]
-#y_train = np.load(training_files[0])["y"]
-
-for np_file in training_files:
-    print(np_file)
-    print(len(np.load(np_file)['x']))
-#    X_train = np.vstack((X_train, np.load(np_file)["x"]))
-#    y_train = np.append(y_train, np.load(np_file)["y"])
-
-#data_save = dict()
-#data_save["samples"] = torch.from_numpy(X_train.transpose(0, 2, 1))
-#data_save["labels"] = torch.from_numpy(y_train)
-#torch.save(data_save, os.path.join(output_dir, "train.pt"))
-
-######## validation files ##########
-validation_files = sorted(list(set(files)-set(pretext_files)-set(training_files))) #list(np.random.choice(sorted(list(set(files)-set(pretext_files)-set(training_files))),32,replace=False))    # left =32
-
-print("\n =========================================== \n")
-print("validation files: ", len(validation_files))
-
-# load files
-#X_train = np.load(validation_files[0])["x"]
-#y_train = np.load(validation_files[0])["y"]
-
-for np_file in validation_files:
-    print(np_file)
-    print(len(np.load(np_file)['x']))
-#    X_train = np.vstack((X_train, np.load(np_file)["x"]))
-#    y_train = np.append(y_train, np.load(np_file)["y"])
-
-
-#data_save = dict()
-#data_save["samples"] = torch.from_numpy(X_train.transpose(0, 2, 1))
-#data_save["labels"] = torch.from_numpy(y_train)
-
-#torch.save(data_save, os.path.join(output_dir, "val.pt"))
-
diff --git a/simclr_sleepedf/config.py b/simclr_sleepedf/config.py
index c05b96e..d4056d6 100644
--- a/simclr_sleepedf/config.py
+++ b/simclr_sleepedf/config.py
@@ -8,6 +8,8 @@ class Config(object):
         self.exp_path = "."
         self.wandb = wandb
         self.batch_size = 128
+        self.eval_batch_size = 512
+        self.eval_early_stopping = 10
 
         self.degree = 0.05
         self.mask_max_points = 200
diff --git a/simclr_sleepedf/models/model.py b/simclr_sleepedf/models/model.py
index 8b12433..a75356d 100644
--- a/simclr_sleepedf/models/model.py
+++ b/simclr_sleepedf/models/model.py
@@ -63,16 +63,15 @@ class sleep_model(nn.Module):
         super(sleep_model,self).__init__()
         
         self.eeg_encoder= encoder()
-        self.weak_pj1 = projection_head(config)
-        self.strong_pj1 = projection_head(config)
+        self.pj = projection_head(config)
 
     def forward(self, weak_data, strong_data):
         
         weak_data = self.eeg_encoder(weak_data)
         strong_data = self.eeg_encoder(strong_data)
 
-        weak_data = self.weak_pj1(weak_data.unsqueeze(1))
-        strong_data = self.strong_pj1(strong_data.unsqueeze(1))
+        weak_data = self.pj(weak_data.unsqueeze(1))
+        strong_data = self.pj(strong_data.unsqueeze(1))
 
         return weak_data, strong_data
 
@@ -119,7 +118,7 @@ class ft_loss(nn.Module):
     def __init__(self, chkpoint_pth, config, device):
 
         super(ft_loss,self).__init__()
-        self.eeg_encoder = encoder(config)
+        self.eeg_encoder = encoder()
         
         chkpoint = torch.load(chkpoint_pth, map_location=device)
         eeg_dict = chkpoint['eeg_model_state_dict']
diff --git a/simclr_sleepedf/trainer.py b/simclr_sleepedf/trainer.py
index 07a7c09..5960406 100644
--- a/simclr_sleepedf/trainer.py
+++ b/simclr_sleepedf/trainer.py
@@ -1,4 +1,5 @@
 import os
+import time, math
 import torch
 import torch.nn as nn
 import matplotlib.pyplot as plt
@@ -70,6 +71,7 @@ class sleep_pretrain(nn.Module):
     def do_kfold(self):
         kfold = KFold(n_splits=5, shuffle=True, random_state=1234)
         k_f1, k_kappa, k_bal_acc, k_acc = 0,0,0,0
+        start = time.time()
         
         for train_idx, test_idx in kfold.split(self.test_subjects):
             test_subjects_train = [self.test_subjects[i] for i in train_idx]
@@ -83,6 +85,9 @@ class sleep_pretrain(nn.Module):
             k_kappa+=kappa
             k_bal_acc+=bal_acc
             k_acc+=acc
+            
+        pit = time.time() - start
+        print(f"Took {int(pit // 60)} min:{int(pit % 60)} secs")
 
         return k_f1/5, k_kappa/5, k_bal_acc/5, k_acc/5
     
@@ -115,22 +120,24 @@ class sleep_pretrain(nn.Module):
                 
             epoch_loss = self.training_epoch_end(outputs)  
             
-            print(f"Pretrain Epoch {epoch}: Epoch Loss {epoch_loss:.6g}")
+            print(f"Epoch Loss {epoch_loss:.6g}")
             
             self.on_epoch_end()
 
             # evaluation step
-            if (epoch % 5 == 0) and (epoch > 10):
+            if (epoch % 5 == 0) :
                 f1, kappa, bal_acc, acc = self.do_kfold()
+
                 self.loggr.log({'F1':f1,'Kappa':kappa,'Bal Acc':bal_acc,'Acc':acc,'Epoch':epoch})
-                
-                if self.max_f1 < f1:
-                    chkpoint = {'eeg_model_state_dict':self.model.model.eeg_encoder.state_dict(),'best_pretrain_epoch':epoch, 'f1': f1}
-                    torch.save(chkpoint, os.path.join(config.exp_path, self.name+ f'_best.pt'))
-                    self.loggr.save(os.path.join(config.exp_path, self.name+ f'_best.pt'))
-                    self.max_f1 = f1
-                
-                
+                print(f'F1: {f1} Kappa: {kappa} B.Acc: {bal_acc} Acc: {acc}')
+
+
+            if self.max_f1 < f1:
+                chkpoint = {'eeg_model_state_dict':self.model.model.eeg_encoder.state_dict(),'best_pretrain_epoch':epoch, 'f1': f1}
+                torch.save(chkpoint, os.path.join(config.exp_path, self.name+ f'_best.pt'))
+                self.loggr.save(os.path.join(config.exp_path, self.name+ f'_best.pt'))
+                self.max_f1 = f1
+                               
 
 class sleep_ft(nn.Module):
     def __init__(self, chkpoint_pth, config, train_dl, valid_dl, pret_epoch, wandb_logger):
@@ -141,17 +148,20 @@ class sleep_ft(nn.Module):
         self.beta1 = config.beta1
         self.beta2 = config.beta2
         self.weight_decay = 3e-5
-        self.batch_size = config.batch_size
+        self.batch_size = config.eval_batch_size
         self.loggr = wandb_logger
         self.criterion = nn.CrossEntropyLoss()
         self.train_ft_dl = train_dl
         self.valid_ft_dl = valid_dl
         self.pret_epoch = pret_epoch
+        self.eval_es = config.eval_early_stopping
         
-        self.max_f1 = torch.tensor(0)
-        self.max_acc = torch.tensor(0)
+        self.best_loss = torch.tensor(math.inf).to(self.device)
+        self.counter = torch.tensor(0).to(self.device)
+        self.max_f1 = torch.tensor(0).to(self.device)
+        self.max_acc = torch.tensor(0).to(self.device)
         self.max_bal_acc = torch.tensor(0)
-        self.max_kappa = torch.tensor(0)
+        self.max_kappa = torch.tensor(0).to(self.device)
         
         self.optimizer = torch.optim.Adam(self.model.parameters(),self.config.lr,betas=(self.config.beta1,self.config.beta2),weight_decay=self.weight_decay)
         self.ft_epoch = config.num_ft_epoch
@@ -177,10 +187,10 @@ class sleep_ft(nn.Module):
         acc = accuracy(outs,y)
         return {'loss':loss.detach(),'acc':acc,'preds':outs.detach(),'target':y.detach()} 
 
-    def validation_epoch_end(self,outputs):   
+    def validation_epoch_end(self, outputs):   
         epoch_preds = torch.vstack([x for x in outputs['preds']])
         epoch_targets = torch.hstack([x for x in outputs['target']])
-        #epoch_loss = torch.hstack([x['loss'] for x in outputs]).mean()
+        epoch_loss = torch.hstack([torch.tensor(x) for x in outputs['loss']]).mean()
         epoch_acc = torch.hstack([torch.tensor(x) for x in outputs['acc']]).mean()
         class_preds = epoch_preds.cpu().detach().argmax(dim=1)
         f1_sc = f1(epoch_preds,epoch_targets,average='macro',num_classes=5)
@@ -195,12 +205,14 @@ class sleep_ft(nn.Module):
             self.max_kappa = kappa
             self.max_bal_acc = bal_acc
             self.max_acc = epoch_acc
+            
+        return epoch_loss
 
     def on_train_end(self):
         return self.max_f1, self.max_kappa, self.max_bal_acc, self.max_acc
 
     def fit(self):
-        for ep in range(self.ft_epoch):
+        for ep in tqdm(range(self.ft_epoch), desc='Evaluation'):
             # Training Loop
             self.model.train()
             ft_outputs = {'loss':[],'acc':[],'preds':[],'target':[]}
@@ -222,8 +234,16 @@ class sleep_ft(nn.Module):
                     ft_outputs['preds'].append(preds)
                     ft_outputs['target'].append(target)
 
-                self.validation_epoch_end(ft_outputs)
-                print(f'FT Epoch: {ep} F1: {self.max_f1.item():.4g} Kappa: {self.max_kappa.item():.4g} \
-                    B.Acc: {self.max_bal_acc.item():.4g} Acc: {self.max_acc.item():.4g}')
-
+                val_loss = self.validation_epoch_end(ft_outputs)
+                
+            if val_loss + 0.001 < self.best_loss:
+                self.best_loss = val_loss
+                self.counter = 0
+            else:
+                self.counter += 1
+     
+            if self.counter == 15:
+                print(f'Early stopped at {ep} epoch')
+                break
+                
         return self.on_train_end()
diff --git a/simclr_sleepedf/ts_main.py b/simclr_sleepedf/ts_main.py
index feb92ea..5f76ff3 100644
--- a/simclr_sleepedf/ts_main.py
+++ b/simclr_sleepedf/ts_main.py
@@ -15,8 +15,8 @@ torch.backends.cudnn.benchmark = False
 np.random.seed(SEED)
 
 
-name = 'simclr_sleepedf'
-ss_wandb = wandb.init(project='new_baselines',name=name,notes='normalized recording wise',save_code=True,entity='sleep-staging')
+name = 'simclr_sleepedf_15'
+ss_wandb = wandb.init(project='me_v2',name=name ,notes='normalized recording wise',save_code=True,entity='sleep-staging')
 config = Config(ss_wandb)
 
 ss_wandb.save('/home/vamsi81523/v2_new_models/simclr_sleepedf//config.py')
